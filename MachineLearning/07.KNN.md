# K-ìµœê·¼ì ‘ ì´ì›ƒ (K-Nearest Neighbors, K-NN)
- ë¶„ë¥˜(Classification)ì™€ íšŒê·€(Regression) ë¥¼ ëª¨ë‘ ì§€ì›

- White box modelì´ë¼ê³  ë³¼ ìˆ˜ ìˆìŒ

- ì˜ˆì¸¡í•˜ë ¤ëŠ” ë°ì´í„°ì™€ input ë°ì´í„°ë“¤ ê°„ì˜ ê±°ë¦¬ë¥¼ ì¸¡ì •í•´ ê°€ì¥ ê°€ê¹Œìš´ Kê°œì˜ ë°ì´í„°ì…‹ì˜ ë ˆì´ë¸”ì„ ì°¸ì¡°í•´ ë¶„ë¥˜/ì˜ˆì¸¡

- í•™ìŠµì‹œ ë‹¨ìˆœíˆ input ë°ì´í„°ë“¤ì„ ì €ì¥ë§Œ í•˜ë©° ì˜ˆì¸¡ ì‹œì ì— ê±°ë¦¬ë¥¼ ê³„ì‚°
    - **í•™ìŠµì€ ë¹ ë¥´ì§€ë§Œ ì˜ˆì¸¡ì‹œ ì‹œê°„ì´ ë§ì´ ì†Œìš”ë¨**
        - ë°ì´í„°ë§Œ ê°€ì§€ê³  ìˆê¸° ë•Œë¬¸ì— fit, ì¦‰ í•™ìŠµí•  ë•ŒëŠ” ì‹œê°„ì´ ì ê²Œ ê±¸ë¦¼
        - í•™ìŠµ ë°ì´í„°ë¥¼ ê°€ì§€ê³  ìˆë‹¤ê°€ predictë¥¼ í•˜ê²Œ ë˜ë©´ ìê¸°ê°€ ê°€ì§€ê³  ìˆëŠ” í•™ìŠµ ë°ì´í„°ì˜ ê±°ë¦¬ë¥¼ ì¬ëŠ” ê²ƒ
        - ì˜ˆì¸¡, ì¦‰ ì„œë¹„ìŠ¤ í•  ë•Œì˜ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦¼


<br>
<br>

## ë¶„ë¥˜

<br>

<p align=center><img src="images/image16.png" width=60%></p>

<br>

- K-NNì—ì„œ **K**ëŠ” ìƒˆë¡œìš´ ë°ì´í„°í¬ì¸íŠ¸ë¥¼ ë¶„ë¥˜í• ë•Œ í™•ì¸í•  ë°ì´í„° í¬ì¸íŠ¸ì˜ ê°œìˆ˜ë¥¼ ì§€ì •í•˜ëŠ” **í•˜ì´í¼íŒŒë¼ë¯¸í„°**

<br>


<p align=center><img src="images/image17.png" width=60%></p>

<br>

- Kë¥¼ 1ë¡œ í•˜ë©´(ì¦‰ ê°€ì¥ ê°€ê¹Œìš´ í•˜ë‚˜ì˜ ì´ì›ƒë§Œ ê³ ë ¤í•˜ë©´) <font color='blue'>íŒŒë€ìƒ‰</font>ìœ¼ë¡œ ë¶„ë¥˜
- Kë¥¼ 3ìœ¼ë¡œ í•˜ë©´ <font color='blue'>ì£¼í™©ìƒ‰</font> ìœ¼ë¡œ ë¶„ë¥˜

<br>

- Kê°€ ë„ˆë¬´ ì‘ìœ¼ë©´ ê³¼ì í•©(Overfitting)ì´ ì¼ì–´ë‚˜ê³  Kê°€ ë„ˆë¬´ í¬ë©´ ì„±ëŠ¥ì´ ë‚˜ë¹ ì§(Underfitting). 
    - Overfitting : Kê°’ì„ ë” í¬ê²Œ ì¡ì•„ì¤Œ
    - Underfitting : Kê°’ì„ ë” ì‘ê²Œ ì¡ì•„ì¤Œ 

    <br>

- K ê°’ì€ ìš°ë¦¬ê°€ ì§€ì •í•´ì¤Œ, ì¦‰ hyper-parameter

<br>
<br>

## ì£¼ìš” í•˜ì´í¼ íŒŒë¼ë¯¸í„°
- ì´ì›ƒ ìˆ˜ 
    - n_neighbors = K
    - **Kê°€ ì‘ì„ ìˆ˜ë¡ ëª¨ë¸ì´ ë³µì¡í•´ì ¸ ê³¼ì í•©ì´ ì¼ì–´ë‚˜ê³  ë„ˆë¬´ í¬ë©´ ë‹¨ìˆœí•´ì ¸ ì„±ëŠ¥ì´ ë‚˜ë¹ ì§**
    - n_neighborsëŠ” Featureìˆ˜ì˜ ì œê³±ê·¼ ì •ë„ë¥¼ ì§€ì •í•  ë•Œ ì„±ëŠ¥ì´ ì¢‹ì€ ê²ƒìœ¼ë¡œ ì•Œë ¤ì ¸ ìˆìŒ
    
    <br>

- ê±°ë¦¬ ì¸¡ì • ë°©ë²• 
    - p=2: ìœ í´ë¦¬ë””ì•ˆ ê±°ë¦¬(Euclidean distance - ê¸°ë³¸ê°’)
    - p=1: ë§¨í•˜íƒ„ ê±°ë¦¬(Manhattan distance)

<br>

> ### ìœ í´ë¦¬ë””ì•ˆ ê±°ë¦¬(Euclidean_distance, L2 norm)

<br>

<p align=center><img src="images/image18.png" width=60%></p>

<br>

<!-- $$
distance = \sqrt{(a_1 - b_1)^2 + (a_2-b_2)^2}\\
nì°¨ì› ë²¡í„°ê°„ì˜ ê±°ë¦¬ = \sqrt{(a_1 - b_1)^2 + (a_2-b_2)^2 +...+(a_n-b_n)^2}
$$ -->

> ### ë§¨í•˜íƒ„ ê±°ë¦¬ (Manhattan distance, L1 norm)

<br>

<p align=center><img src="images/image19.png" width=60%></p>


<br>

<!-- $$
distance = |a_1 - b_1| + |a_2 - b_2| \\
ğ‘›ì°¨ì›ë²¡í„°ê°„ì˜ê±°ë¦¬= |a_1 - b_1| + |a_2 - b_2| + ... + |a_n - b_n|
$$ -->

<br>
<br>


## ìš”ì•½
- K-NNì€ ì´í•´í•˜ê¸° ì‰¬ìš´ ëª¨ë¸ì´ë©° íŠœë‹í•  í•˜ì´í¼íŒŒë¼ë¯¸í„°ì˜ ìˆ˜ê°€ ì ì–´ ë¹ ë¥´ê²Œ ë§Œë“¤ ìˆ˜ ìˆìŒ
- K-NNì€ ì„œë¹„ìŠ¤í•  ëª¨ë¸ì„ êµ¬í˜„í• ë•Œ ë³´ë‹¤ëŠ” **ë³µì¡í•œ ì•Œê³ ë¦¬ì¦˜ì„ ì ìš©í•´ ë³´ê¸° ì „ì— í™•ì¸ìš© ë˜ëŠ” base lineì„ ì¡ê¸° ìœ„í•œ ëª¨ë¸ë¡œ ì‚¬ìš©**
- í›ˆë ¨ì„¸íŠ¸ê°€ ë„ˆë¬´ í° ê²½ìš°(Featureë‚˜ ê´€ì¸¡ì¹˜ì˜ ê°œìˆ˜ê°€ ë§ì€ ê²½ìš°) ê±°ë¦¬ë¥¼ ê³„ì‚°í•˜ëŠ” ì–‘ì´ ëŠ˜ì–´ë‚˜ ì˜ˆì¸¡ì´ ëŠë ¤ì§
- Featureê°„ì˜ ê°’ì˜ ë‹¨ìœ„ê°€ ë‹¤ë¥´ë©´ ì‘ì€ ë‹¨ìœ„ì˜ Featureì— ì˜í–¥ì„ ë§ì´ ë°›ê²Œ ë˜ë¯€ë¡œ **ì „ì²˜ë¦¬ë¡œ Scalingì‘ì—…**ì´ í•„ìš”!!
- Featureê°€ ë„ˆë¬´ ë§ì€ ê²½ìš°ì™€ ëŒ€ë¶€ë¶„ì˜ ê°’ì´ 0ìœ¼ë¡œ êµ¬ì„±ëœ(í¬ì†Œ-sparse) ë°ì´í„°ì…‹ì—ì„œ ì„±ëŠ¥ì´ ì•„ì£¼ ë‚˜ì¨!!
    - ë²”ì£¼í˜• ë°ì´í„°ë¥¼ ë‹¤ë£° ë•Œ **Label Encoding**ì„ í•´ì•¼í•¨!

<br>

- ì½”ë“œ

    ```python
    from sklearn.datasets import load_breast_cancer
    from sklearn.model_selection import train_test_split
    from sklearn.model_selection import GridSearchCV
    from sklearn.neighbors import KNeighborsClassifier
    from sklearn.preprocessing import LabelEncoder
    from sklearn.preprocessing import MinMaxScaler
    from sklearn.metrics import accuracy_score
    from sklearn.pipeline import Pipeline

    # ë°ì´í„° ë¡œë“œ ë° ë¶„ë¥˜
    X, y = load_breast_cancer(return_X_y = True)
    X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=0)

    # íŒŒì´í”„ë¼ì¸ ìƒì„±
    order = [
        ('scaler', MinMaxScaler()),
        ('knn', KNeighborsClassifier())
    ]

    pipeline = Pipeline(order, verbose=True)

    # GridSearch
    param = {
        "knn__n_neighbors" : range(1, 11),
        "knn__p" : [1, 2]
    }

    gs = GridSearchCV(pipeline, param, scoring="accuracy", cv = 5, n_jobs=-1)
    gs.fit(X_train, y_train)

    result_df = pd.DataFrame(gs.cv_results_)
    result_df[result_df.columns[6:]].sort_values('rank_test_score')

    best_model = gs.best_estimator_
    print(accuracy_score(y_test, best_model.predict(X_test)))
    ```
