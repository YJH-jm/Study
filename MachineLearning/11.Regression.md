# 회귀(Regression)
지도 학습(Supervised Learning)으로 예측할 Target이 연속형데이터인 경우

<br>
<br>

## 회귀의 주요 평가 지표
예측값과 실제 값간의 차이를 구함

<br>
<br>


### MSE (Mean Squared Error)
- 실제 값과 예측값의 차를 제곱해 평균 낸 것
- `mean_squared_error(y, pred_y)` 
- `neg_mean_squared_error`
- 오차가 제곱된 큰 값으로 나옴

    <br>

    $$MSE=\frac{1}{n} \sum_{i=1}^{n}(y_{i}-\hat{y_{i}})^{2}$$  
    
    <br>

    - $y_{i}$ : 실제값
    - $\hat{y_{i}}$ : 모델이 예측한 값  

<br>
<br>

### RMSE (Root Mean Squared Error)
- MSE의 제곱근
    - MSE는 오차의 제곱한 값이므로 실제 오차의 평균보다 큰 값  
- scikit-learn은 함수를 지원하지 않음 
    - MSE를 구한 뒤 np.sqrt()로 제곱근을 구함


<br>
<br>

### $R^{2}$ (R square, 결정계수)
- 평균으로 예측했을 때 오차(총오차) 보다 모델을 사용했을 때 얼마 만큼 더 좋은 성능을 내는지를 비율로 나타낸 값 
- 내가 만든 모델이 평균으로 예측했을 때 보다 얼마나 좋은지
- 1에 가까울 수록 좋은 모델.
- `r2_score()`
- 'r2'
- [참고](https://ko.khanacademy.org/math/statistics-probability/describing-relationships-quantitative-data/assessing-the-fit-in-least-squares-regression/a/r-squared-intuition)

    <br>

    $$R^2 = \frac{\sum_{i=1}^{n}(\hat{y_i}-\bar{y})^2}{\sum_{i=1}^{n}(y_i - \bar{y})^2}$$

    <br>

    - $y_i$ : i번째 실제 값
    - $\hat{y_i}$ : i번째 예측 값
    - $\bar{y}$ : y의 평균    

<br>

- 코드
    ```python
    import numpy as np
    import matplotlib.pyplot as plt
    from sklearn.datasets import make_regression
    from sklearn.model_selection import cross_val_score
    from sklearn.linear_model import LinearRegression
    from sklearn.metrics import mean_squared_error, r2_score

    # 회귀문제에 사용할 수 있는 가상의 데이터셋을 원하는 조건을 설정해서 생성하는 함수
    X, y = make_regression(n_samples = 100, # 데이터의 개수
                        n_features= 1, #featue(컬럼) 개수
                        n_informative=1, # target(Label)에 영향을 주는 feature의 개수,
                        noise = 30, # 잡음, 모델이 찾을 수 없는 값의 범위,
                        random_state = 0)

    plt.scatter(X, y)
    plt.show()

    # LinearRegression 모델을 이용해서 추론 후 모델 평가
    lr = LinearRegression()
    lr.fit(X, y)
    pred = lr.predict(X)

    mse = mean_squared_error(y, pred)
    rmse = np.sqrt(mse)
    r2 = r2_score(y, pred)

    print(f"MSE : {mse}, RMSE : {rmse}, R2 : {r2}" )

    # Cross validation1
    lr2 = LinearRegression()
    score_list = cross_val_score(lr2, # 모델
                                X,
                                y,
                                scoring="r2", # 생략가능, 생략하면 분류일 때는 accuracy, 회귀모델일 경우는 r2(R square)
                                cv=3 # fold의 개수
                                )
    print(score_list) # fold별 검증 결과
    print(np.mean(score_list))

    # Cross validation2
    lr3 = LinearRegression()
    score_list2 = cross_val_score(lr3, #모델
                                X, y,
                                cv = 3, #fold의 개수
                                scoring="neg_mean_squared_error", # MES * -1,gridsearch 때문에 -1을 하게 됨 -> 등수를 매기는데 오차이기 때문에 값이 작을 수록 좋은데 클수록 결과가 높게 나오게 됨
                                )

    print(score_list2*-1)
    print(np.mean(score_list2)*-1)

    # LR.coef _ (Feature별 가중치), LR.intercept_(biae)
    print(lr.coef_, lr.intercept_)

    # 에측값 돌려주는 함수
    def my_pred(X):
        return lr.coef_[0] * X + lr.intercept_

    # X, y와 예측 결과를 시각화
    plt.figure(figsize=(7, 6))
    plt.scatter(X, y)
    y_hat = my_pred(X) # lr.predict(X)
    plt.plot(X, y_hat, color = 'red')
    plt.show()
    ```

<br>

- 코드
    - 기존 분류 모델의 회귀 모델

    ```python
    import numpy as np
    from sklearn.datasets import make_regression
    from sklearn.model_selection import train_test_split
    from sklearn.neighbors import KNeighborsRegressor
    from sklearn.tree import DecisionTreeRegressor
    from sklearn.ensemble import RandomForestRegressor
    from sklearn.ensemble import GradientBoostingRegressor
    from sklearn.ensemble import VotingRegressor
    from sklearn.svm import SVR
    from sklearn.metrics import mean_squared_error
    from sklearn.metrics import r2_score

    X, y = make_regression(n_samples=100, n_features=1, n_informative=1, noise=30, random_state=0)
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

    knn = KNeighborsRegressor()
    tree = DecisionTreeRegressor()
    rf = RandomForestRegressor()
    gb = GradientBoostingRegressor()
    svr = SVR()

    estimators = [
        ("knn", knn),
        ("tree", tree),
        ("rf", rf),
        ("gb", gb),
        ("svr", svr)
    ]

    # 
    def print_regression_metrics(y, pred, title=None):
        mse = mean_squared_error(y, pred)
        rmse = np.sqrt(mse)
        r2 = r2_score(y, pred)
        
        if title:
            print(title)
        print(f"MSE : {mse}, RMSE : {rmse}, R2 : {r2}")
        

    for model_name, model in estimators:
        # 학습
        model.fit(X_train, y_train)
        
        # 평가
        pred_train = model.predict(X_train)
        pred_test = model.predict(X_test)
        
        print_regression_metrics(y_train, pred_train, title=f"{model_name}-Train")
        print_regression_metrics(y_test, pred_test, title=f"{model_name}-Test")
        
        print("-"*100)
        
    vote = VotingRegressor(estimators)
    vote.fit(X_train, y_train)

    print_regression_metrics(y_train, vote.predict(X_train), title = "Voting Train")
    print_regression_metrics(y_test, vote.predict(X_test), title = "Voting Test")
    ```

<br>
<br>

# 선형회귀 개요
- 종속 변수 y와 한 개 이상의 독립 변수 (또는 설명 변수) X와의 선형 상관 관계를 모델링하는 기법

<br>
<br>

## 선형회귀 모델

<br>

$$\hat{y_i} = w_1 x_{i1} + w_2 x_{i2} + ... + w_{p} x_{ip} + b $$

<br>

- $\hat{y_i}$ 예측값
- $x$ : 특성(feature-컬럼)
- $w$ : 가중치(weight), 회귀계수(regression coefficient), 특성이 $\hat{y_i}$ 에 얼마나 영향을 주는지 정도
- $b$ : 절편
- $p$ : p 번째 특성(feature)/p번째 가중치
- $i$ : i번째 관측치(sample)

<br>
<br>


## 손실(loss)함수/오차(error)함수/비용(cost)함수/목적(objective)함수
- 모델이 출력한 예측값과 실제 값 사이의 차이를 계산하는 함수
- 평가 지표로 사용되기도 하고 모델을 최적화하는데 사용


<br>
<br>

## 최적화(Optimize)
- 손실함수의 값이 최소화 되도록 모델을 학습하는 과정
- 최적화의 두가지 방법
    - 정규방정식
    - 경사하강법


<br>
<br>

## LinearRegression
- 가장 기본적인 선형 회귀 모델
- Feature 전처리
    - 범주형 : one hot encoding
    - 연속형 : Feature Scaling
        - StandardScaler를 사용하는 경우 성능이 더 잘 나오는 경향

<br>

- 코드
    ```python
    import pandas as pd
    import numpy as np
    from sklearn.datasets import load_boston
    from sklearn.model_selection import train_test_split
    from sklearn.preprocessing import StandardScaler
    from sklearn.linear_model import LinearRegression
    from sklearn.metrics import mean_squared_error, r2_score
    import matplotlib.pyplot as plt
    # 데이터 로드 및 확인
    data = load_boston()
    data.keys()
    X, y = data['data'], data['target']
    df = pd.DataFrame(X, columns=data['feature_names'])

    df.shape
    df.head()
    df.info()
    df.isnull().sum()
    df['CHAS'].value_counts()

    # 데이터 전처리
    chas_df = pd.get_dummies(df['CHAS'])
    chas_df.columns = ['CHAS_0', 'CHAS_1']
    df2 = df.join(chas_df) # CHAS 칼럼 onehotencoding 한 컬럼 추가
    df2.drop(columns='CHAS', inplace=True) # CHAS 컬럼 제거 후 원본을 변환

    X = df2

    # 데이터 나누기
    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)
    X_train.shape, X_test.shape

    # 데이터 전처리
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    # 평가지표 출력함수
    def print_regression_metrics(y, pred, title=None):
        mse = mean_squared_error(y, pred)
        rmse = np.sqrt(mse)
        r2 = r2_score(y, pred)
        
        if title:
            print("title")
        
        print(f"MSE : {mse}, RMSE : {rmse}, R2 : {r2}")
        print("-"*50)
        
    # 모델 생성, 학습
    lr = LinearRegression()
    lr.fit(X_train_scaled, y_train)

    # 가중치, 절편 확인
    print(lr.coef_, lr.intercept_)
    coef = pd.Series(lr.coef_, index = X_train.columns)

    # 모델평가
    pred_train = lr.predict(X_train_scaled)
    pred_test= lr.predict(X_test_scaled)
    print_regression_metrics(y_train, pred_train, title="Train")
    print_regression_metrics(y_test, pred_test, title="Test")

    # 실제값과 예측값을 plotting
    plt.figure(figsize=(15, 7))
    plt.plot(range(len(y_test)), y_test, label = "MEDV", marker = "x")
    plt.plot(range(len(y_test)), pred_test, label = "Pred", marker="o")
    plt.legend()
    plt.plot()
    ```

<br>
<br>

# 다항 회귀(Polynomial Regression)
- 단순한 직선형 보다는 복잡한 비선형 형태의 데이터를 추론하기 위한 모델
- 특별한 모델이 있는 것이 아니라 데이터셋을 변환시켜줘야 함
- Feature들을 거듭제곱한 Feature들을 추가하여 모델링 진행
- PolynomialFeatures 변환기를 이용

- multifeature 개념이 아닌, 임의로 feature를 늘리는 것

<br>

- 코드
    - PolynomialFeatures를 Boston set에 적용
    ```python
    import pandas as pd
    import numpy as np
    from sklearn.datasets import load_boston
    from sklearn.model_selection import train_test_split
    from sklearn.preprocessing import StandardScaler
    from sklearn.preprocessing import PolynomialFeatures
    from sklearn.linear_model import LinearRegression
    from sklearn.metrics import mean_squared_error, r2_score
    import matplotlib.pyplot as plt
    # 데이터 로드 및 확인
    data = load_boston()
    data.keys()
    X, y = data['data'], data['target']
    df = pd.DataFrame(X, columns=data['feature_names'])

    df.shape
    df.head()
    df.info()
    df.isnull().sum()
    df['CHAS'].value_counts()

    # 데이터 전처리
    chas_df = pd.get_dummies(df['CHAS'])
    chas_df.columns = ['CHAS_0', 'CHAS_1']
    df2 = df.join(chas_df) # CHAS 칼럼 onehotencoding 한 컬럼 추가
    df2.drop(columns='CHAS', inplace=True) # CHAS 컬럼 제거 후 원본을 변환

    X = df2

    # 데이터 나누기
    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)
    X_train.shape, X_test.shape

    # 데이터 전처리
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    # 평가지표 출력함수
    def print_regression_metrics(y, pred, title=None):
        mse = mean_squared_error(y, pred)
        rmse = np.sqrt(mse)
        r2 = r2_score(y, pred)
        
        if title:
            print(title)
        
        print(f"MSE : {mse}, RMSE : {rmse}, R2 : {r2}")
        print("-"*50)
        
    # 모델 생성, 학습
    pnf = PolynomialFeatures(degree=2,# degree : 최고차항을 지정
                            include_bias=False) # True -> 상수항을 추가(모든 값이 1인 Featrue 추가)
    X_train_scaled_poly = pnf.fit_transform(X_train_scaled)
    X_test_scaled_poly = pnf.transform(X_test_scaled)
    print(X_train_scaled.shape, X_train_scaled_poly.shape)
    lr = LinearRegression()
    lr.fit(X_train_scaled_poly, y_train)

    # 가중치, 절편 확인
    print(lr.coef_, lr.intercept_)

    # 모델평가
    pred_train = lr.predict(X_train_scaled_poly)
    pred_test = lr.predict(X_test_scaled_poly)
    print_regression_metrics(y_train, pred_train, title="Train")
    print_regression_metrics(y_test, pred_test, title="Test")

    # 실제값과 예측값을 plotting
    plt.figure(figsize=(15, 7))
    plt.plot(range(len(y_test)), y_test, label = "MEDV", marker = "x")
    plt.plot(range(len(y_test)), pred_test, label = "Pred", marker="o")
    plt.legend()
    plt.plot()
    ```

<br>
<br>

## 규제 (Regularization)
- 선형 회귀 모델에서 과적합(overfitting) 문제를 해결하기 위해 가중치(회귀계수)에 페널티 값을 적용하는 것
- 입력데이터의 Feature들이 너무 많은 경우 과적합이 발생
    - Feature수에 비해 관측치 수가 적은 경우 모델이 복잡해 지면서 과적합이 발생

<br>

- 해결
    - 데이터를 더 수집 
    - Feature selection
        - 불필요한 Features들을 제거
    - 규제 (Regularization) 통해 Feature들에 곱해지는 가중치가 커지지 않도록 제한

<br>
<br>

## Ridge Regression
- 손실함수(loss function)에 규제항으로 $\alpha \sum_{i=1}^{n}{w_{i}^{2}}$ (L2 Norm)을 더해줌
- $\alpha$ 는 하이퍼파라미터로 모델을 얼마나 많이 규제할지 조절
    - $\alpha = 0$ 에 가까울수록 규제가 약해짐 
        - 0일 경우 선형 회귀동일
    - $\alpha$ 가 커질 수록 모든 가중치가 작아져 입력데이터의 Feature들 중 중요하지 않은 Feature의 output에 대한 영향력이 작아짐

<br>
<br>

- MES에 규제항을 더해줌
    - 규제항 
        - 오차를 키워줌
        - $w$ 영향력을 작게 만들어 feature의 영향력을 낮춤
    - 오차는 $w$를 잘못찾아서 발생
    - $\alpha$ 는 control 할 수 있는 hyper-parameter
    - 규제를 통해 feature를 selection하는 것이 목적! -> 오차를 크게 만들어줌 -> 그러기 위해서 w를 작게 만들어줌


<br>

$$\text{Loss Function}(w) = \text{MSE}(w) + \alpha \cfrac{1}{2}\sum_{i=1}^{n}{w_{i}^{2}}$$

<br>

- 0에 가깝게는 되는데 절대 0이 되지 않음
- 보통 Lasso 보다는 Ridge를 많이 사용

<br>
<br>

## Lasso(Least Absolut Shrinkage and Selection Operator) Regression

- 손실함수에 규제항으로 $\alpha \sum_{i=1}^{n}{\left| w_i \right|}$ (L1 Norm) 더함
- Lasso 회귀의 상대적으로 덜 중요한 특성의 가중치를 0으로 만들어 자동으로 Feature Selection 만들어줌

<br>

$$\text{Loss Function}(w) = \text{MSE}(w) + \alpha \sum_{i=1}^{n}{\left| w_i \right|}$$

- 특정 feature들이 0으로 바뀌는 것이 가능

<br>

- 코드
    - Ridge, Lasso

    <br>

    ```python
    import pandas as pd
    from sklearn.datasets import load_boston
    from sklearn.model_selection import train_test_split, GridSearchCV
    from sklearn.preprocessing import StandardScaler
    from sklearn.linear_model import Ridge, Lasso

    X, y = load_boston(return_X_y = True)
    X_trian, X_test, y_train, y_test = train_test_split(X, y, random_state=0)

    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.fit_transform(X_test)

    param = {
        "alpha" : [0.01, 0.1, 1, 10, 100]
    }

    # Ridge
    ridge = Ridge(random_state=0)
    gs_ridge = GridSearchCV(ridge, param, cv=5, scoring=["r2" , "neg_mean_squared_error"], refit="r2")
    gs_ridge.fit(X_train_scaled, y_train)
    best_model_ridge = gs_ridge.best_estimator_

    print_regression_metrics(y_train, best_model_ridge.predict(X_train_scaled), "Ridge Train")
    print_regression_metrics(y_test, best_model_ridge.predict(X_test_scaled), "Ridge Test")
    print(gs_ridge.best_params_)

    # Lasso
    lasso = Lasso(random_state=0)
    gs_lasso = GridSearchCV(lasso, param, cv=5, scoring=["r2" , "neg_mean_squared_error"], refit="r2")
    gs_lasso.fit(X_train_scaled, y_train)
    best_model_lasso = gs_lasso.best_estimator_

    print_regression_metrics(y_train, best_model_lasso.predict(X_train_scaled), "Lasso Train")
    print_regression_metrics(y_test, best_model_lasso.predict(X_test_scaled), "Lasso Test")
    print(gs_lasso.best_params_)

    ```

<br>
<br>


## 엘라스틱넷
- 릿지와 라쏘를 절충한 모델
- 규제항에 릿지, 회귀 규제항을 더해서 추가
- 혼합뷰율 $r$ 을 사용해 혼합정도를 조절
- $r=0$ 이면 릿지와 같고 $r=1$ 이면 라쏘와 같음


<br>

$$\text{Loss Function}(w) = \text{MSE}(w) + r\alpha \sum_{i=1}^{n}{\left| w_i \right|} + \cfrac{1-r}{2}\alpha\sum_{i=1}^{n}{w_{i}^{2}}$$

<br>

- 코드
    - 전처리 및 데이터 Lasso, Ridge 코드 참고 
    
    <br>

    ```python
    from sklearn.linear_model import ElasticNet
    elastic = ElasticNet(alpha=0.1, l1_ratio=0.4) # L1규제(Lasso) 비율 : 0.4, L2규제(Ridge) 비율 : 0.6
    elastic.fit(X_train_scaled, y_train)

    pred_train = elastic.predict(X_train_scaled)
    pred_test = elastic.predict(X_test_scaled)

    print_regression_metrics(y_train, pred_train, title="Train")
    print_regression_metrics(y_test, pred_test, title="Test")
    ```

<br>

# 정리
- 일반적으로 선형회귀의 경우 어느정도 규제가 있는 경우가 성능이 좋음
- 기본적으로 **Ridge** 사용
- Target에 영향을 주는 Feature가 몇 개뿐일 경우 특성의 가중치를 0으로 만들어 주는 **Lasso** 사용 
- 특성 수가 학습 샘플 수 보다 많거나 feature간에 연관성이 높을 때는 **ElasticNet** 사용

<br>
<br>

# 로지스틱 회귀 (LogisticRegression)
- 선형회귀 알고리즘을 이용한 이진 분류 모델
- Sample이 특정 클래스에 속할 확률을 추정 
    
<br>
<br>

## 확률 추정
- 선형회귀처럼 입력 특성(Feature)에 가중치 합을 계산한 값을 로지스틱 함수를 적용해 확률을 계산

<br>

$$\hat{p} = \sigma \left( \mathbf{W}^{T} \cdot \mathbf{x} \right)$$

<br>



- $\sigma()$ : logistic함수
- $\mathbf{W}$ : 가중치
- $\mathbf{x}$ : 입력 특성

<br>

-  **로지스틱 함수**
    - 0과 1사이의 실수를 반환
    - S 자 형태의 결과를 내는 **시그모이드 함수(sigmoid function)** 

    <br>


    $$\sigma(x) = \frac{1}{1 + \mathbf{e}^{-x}}$$

    <br>



- 샘플 **x**가 양성에 속할 확률

    <br>

    $$\hat{y} = \begin{cases} 0\quad\hat{p}<0.5\\1\quad\hat{p}\geqq0.5 \end{cases}$$

    <br>
<br>

## 손실 함수(Loss Function)
- LogisticRegression의 전체 데이터 셋에 대한 손실함수 
    - **로그 손실(log loss)**, **Binary Cross Entropy** 
        - 회귀모델이 아닌 이진분류이기 때문

    <br>

    $$L(\mathbf{W}) = - \frac{1}{m} \sum_{i=1}^{m}{\left[ y_{i} \log{\left( \hat{p}_i \right)} + \left( 1 - y_i \right) \log{\left( 1 - \hat{p}_i \right)} \right]}$$

    <br>

    - $y$ : 실제값
    - $\hat{p}$ : 예측확률

<br>

- 손실이 커져야하기 때문에 -를 곱해줌
- y(실제값) 이 1인 경우
    - $y_{i}\log{\left(\hat{p}_i\right)}$ 이 손실을 계산
    - 맞았다면 $\hat{p}$ 이 0.5 이상
- y가 0인 경우 
    -  $\left( 1 - y_i \right) \log{\left( 1 - \hat{p}_i \right)}$ 이 손실을 계산
- $\hat{p}$ (예측확률)이 클수록 반환값은 작아지고 작을 수록 값이 커짐

<br>
<br>

## 최적화 
- 위 손실을 가장 적게하는 W(가중치) 찾아야 함
- 로그 손실함수는 최소값을 찾는 정규방적식이 없기 때문에 **LogisticRegression은 경사하강법을 이용해 최적화를 진행**
- 로그 손실을 $\mathbf{W}$로 미분
    - 아래 도함수로 기울기를 구해 기울기가 0이 될 때 까지 W(가중치)들을 update

<br>

$$\frac{\partial}{\partial w_j} = \frac{1}{m} \sum_{i=1}^{m}{\left( \sigma \left( \mathbf{W}^{T} \cdot \mathbf{x}_i \right) - \mathbf{y}_i \right)} x_{ij}$$

<br>
<br>

## LogisticRegression 주요 하이퍼파라미터
- penalty: 과적합을 줄이기 위한 규제방식
    - `l1`, `l2`(기본값), `elasticnet`, `none` 
- C: 규제강도(기본값 1)
- max_iter(기본값 100)
    - 경사하강법 반복횟수

<br>

- 코드
    - logistic regression

    <br>

    ```python
    import pandas as pd
    from sklearn.datasets import load_breast_cancer
    from sklearn.preprocessing import StandardScaler
    from sklearn.model_selection import train_test_split, GridSearchCV
    from sklearn.linear_model import LogisticRegression
    from sklearn.metrics import accuracy_score

    X, y = load_breast_cancer(return_X_y=True)
    X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=0)

    # Scaling
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.fit_transform(X_test)

    # 모델 생성
    lr = LogisticRegression(random_state=0)
    param = {
        'penalty' : ['l1', 'l2'],
        'C' : [0.001, 0.01, 0.1, 1, 10]
    }
    gs = GridSearchCV(lr, param, cv=5,scoring='accuracy', n_jobs=-1)

    gs.fit(X_train_scaled, y_train)

    result = pd.DataFrame(gs.cv_results_)
    result.sort_values('rank_test_score').head()
    print(gs.best_params_)
    best_model = gs.best_estimator_
    pred_test = best_model.predict(X_test_scaled)
    print(accuracy_score(y_test, pred_test))
    ```
