{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2ac2f3f",
   "metadata": {},
   "source": [
    "## Pretrained ResNet18 ImageNet Test\n",
    "\n",
    "Pytorch Hub 공식 ResNet18 아키텍쳐를 이용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0739aa6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# python library \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2ce2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU 장치사용 설정\n",
    "use_cuda = True\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84ef35c",
   "metadata": {},
   "source": [
    "#### ImageNet에 정의된 클래스 정보 가지고 오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc0d4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlretrieve, Request, urlopen\n",
    "import json\n",
    "\n",
    "# 이미지넷(ImageNet) 에 정의된 1000개의 클래스 정보 가지고 오기\n",
    "# imagenet_json, _ = urlretrieve('https://www.anishathalye.com/media/2017/07/25/imagenet.json' )\n",
    "# 위의 코드 403 ERROR 발생\n",
    "req = Request('http://www.anishathalye.com/media/2017/07/25/imagenet.json', headers={'User-Agent': 'Mozilla/5.0'})\n",
    "imagenet_json= urlopen(req).read()\n",
    "\n",
    "imagenet_labels = json.loads(imagenet_json)\n",
    "imagenet_labels[18]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4bc9d88",
   "metadata": {},
   "source": [
    "#### 이미지 처리 함수 정의 및 이미지 가져와 출력해보기\n",
    "ResNet은 일반적으로 이미지에 대하여 **Resize, CenterCrop, ToTensor()**와 **입력 데이터 정규화**를 사용하는 모델 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6a3a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256), # 이미지 크기 변경\n",
    "    transforms.CenterCrop(224), # 이미지의 중앙 부분을 잘라서 크기 조절\n",
    "    transforms.ToTensor(), # torch.Tensor 형식으로 변경 [0, 255] -> [0, 1]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6788c30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import PIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a3ff88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 특정한 경로에서 이미지를 가져와 torch.Tensor로 변환하는 함수\n",
    "\n",
    "def image_loader(path):\n",
    "    image = PIL.Image.open(path)\n",
    "    # 전처리 이후 네트워크 입력에 들어갈 이미지에 배치 목적의 차원 추가\n",
    "    image = preprocess(image).unsqueeze(0)\n",
    "    return image.to(device, torch.float) # GPU에 올리기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8511ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실제로 특정 URL에서 이미지를 불러오기 (얼룩 고양이)\n",
    "# url = \"http://www.image-net.org/nodes/10/02123045/6c/6c34fe7c9d846c33a2f1a9b47a766b44ab4ec70d.thumb\"\n",
    "# image_path, _ = urlretrieve(url)\n",
    "# image = image_loader(image_path)\n",
    "# 위의 경로 없어짐\n",
    "\n",
    "image_path = \"./data/cat_.jpeg\"\n",
    "image = image_loader(image_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48045c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(tensor):\n",
    "    # matplotlib는 CPU 기반이므로 CPU로 옮기기\n",
    "    image = tensor.cpu().clone()\n",
    "    # torch.Tensor에서 사용되는 배치 목적의 차원(dimension 제거)\n",
    "    image = image.squeeze(0)\n",
    "    # PIL 객체로 변경\n",
    "    image = transforms.ToPILImage()(image)\n",
    "    # 이미지를 화면에 출력, matplotlib는 [0, 1] 사이의 값이라도 정상적으로 이미지 출력 처리\n",
    "    plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0adc5b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360176c6",
   "metadata": {},
   "source": [
    "#### Pretrained 된 모델 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2b08e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력 데이터 정규화를 위한 클래스 정의\n",
    "\n",
    "class Normalize(nn.Module) :\n",
    "    def __init__(self, mean, std) :\n",
    "        super(Normalize, self).__init__()\n",
    "        self.register_buffer('mean', torch.Tensor(mean))\n",
    "        self.register_buffer('std', torch.Tensor(std))\n",
    "        \n",
    "    def forward(self, input):\n",
    "        mean = self.mean.reshape(1, 3, 1, 1)\n",
    "        std = self.std.reshape(1, 3, 1, 1)\n",
    "        return (input - mean) / std"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c77b48b",
   "metadata": {},
   "source": [
    "**nn.Module.register_buffer**\n",
    "\n",
    "<br>\n",
    "\n",
    "일반적으로 모델 매개별수로 간주되지 않는 버퍼를 등록할 때사용\n",
    "ex> BatchNorm에서 \"running_mean\"은 매개 변수는 아니지만 상태로서 사용 가능\n",
    " \n",
    "```\n",
    "Args:\n",
    "    name (string): name of the buffer. The buffer can be accessed\n",
    "        from this module using the given name\n",
    "    tensor (Tensor): buffer to be registered.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1936b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential( # 여러개의 layer를 순서대로 실행하기 위해 사용\n",
    "    # 기본적인 ResNet18과 동일한 동작을 위하여 정규화 레이어 추가\n",
    "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "#     torch.hub.load('pytorch/vision:v0.6.0', 'resnet18', pretrained=True)\n",
    "    torch.hub.load('pytorch/vision:v0.6.0', 'resnet18', weights=models.ResNet18_Weights.DEFAULT)\n",
    "    ).to(device).eval() # 모델을 GPU로 옮기고 평가 모드로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3293a6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기본적인 이미지를 실제 모델에 넣어 결과 확인\n",
    "outputs = model(image)\n",
    "\n",
    "# 확률을 계산하기 위해 softmax 함수\n",
    "percentages = torch.nn.functional.softmax(outputs, dim=1)[0] * 100\n",
    "\n",
    "# 가장 높은 값을 가지는 5개의 인덱스를 하나씩 확인 \n",
    "for i in outputs[0].topk(5)[1]:\n",
    "    print(f\"인덱스 : {i.item()} / 클래스명 : {imagenet_labels[i]} / 확률 : {round(percentages[i].item(), 4)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d993e91",
   "metadata": {},
   "source": [
    "## CIFAR10 데이터 학습\n",
    "- ResNet18 모델을 이용하여 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d236d3ba",
   "metadata": {},
   "source": [
    "#### ResNet18 모델 정의 및 인스턴스 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3234c70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import os\n",
    "\n",
    "# ResNet18을 위해 최대한 간단히 수정한 BasicBlock 클래스 정의\n",
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        \n",
    "        # 3x3 필터를 사용 (너비와 높이를 줄일 때는 stride 값 조절)\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes) \n",
    "        \n",
    "        # padding을 1 주기 대문에 높이와 너비가 동일\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        \n",
    "        self.shortcut = nn.Sequential() # identity인 경우\n",
    "        if stride != 1: # stride가 1이 아니라면, identity mapping이 아닌 경우\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes)\n",
    "            )\n",
    "            \n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x) # skip connection\n",
    "        out = F.relu(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    \n",
    "# ResNet Class 정의\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "        \n",
    "        # 64개의 3x3 필터 (filter)를 사용\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1) \n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(512, num_classes)\n",
    "\n",
    "        \n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks-1) # stride 값이 맨 처음 convolution 연산에만 적용될 수 있도록 ex > layer1 : [1, 1]\n",
    "        # 즉, 첫 번째 convolution 연산에 의해서만 높이와 너비가 줄어들 수 있음 \n",
    "        \n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes # 다음 레이어를 위해 채널 수 변경\n",
    "            \n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "\n",
    "# ResNet18 함수 정의\n",
    "def ResNet18():\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860555ee",
   "metadata": {},
   "source": [
    "#### Dataset 다운로드 및 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1690e870",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "transforms_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "transforms_test = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=transforms_train)\n",
    "test_dataset = torchvision.datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=transforms_test)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=4)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=100, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a751c09e",
   "metadata": {},
   "source": [
    "#### 환경 설정 및 학습 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51174b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "\n",
    "net = ResNet18()\n",
    "net = net.to(device)\n",
    "net = torch.nn.DataParallel(net) \n",
    "# torch.nn.DataParallel 같은 모델을 지정한 여러 gpu device에 복사하고 입력 배치를 gpu 별로 쪼개 각 gpu 별로 forward/backward 연산 수행\n",
    "cudnn.benchmark=True \n",
    "# cudnn.bebchmark가 True인 경우 cudnn의 benchmark를 통해 최적의 backend연산을 찾는 flag를 True로 하겠다는 의미\n",
    "\n",
    "learning_rate = 0.1 # CIFAR10 dataset은 통상적으로 0.1부터 시작하여 줄여가는 방식 사용\n",
    "file_name = \"resnet18_cifar10.pt\"\n",
    "\n",
    "criterion=nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=learnng_rate, momentum=0.9, weight_decay=0.0002)\n",
    "\n",
    "def train(epoch):\n",
    "    print(\"\\n[ Train epoch : %d ]\" % epoch)\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        benign_outputs = net(inputs)\n",
    "        loss = criterion(benign_outputs,targets)\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step() # 전체 모델 parameter update\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = benign_outputs.max(1)\n",
    "        \n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "        \n",
    "        if batch_idx % 100 == 0:\n",
    "            print('\\nCurrent batch : ', str(batch_idx))\n",
    "            print(\"Current benign train accuracy : \", str(predicted.eq(targets).sum().item() / targets.size(0)))\n",
    "            print(\"Current benign train loss : \", loss.item())\n",
    "            \n",
    "    print(\"\\Total benign train accuracy : \", 100 * correct / total)\n",
    "    print(\"Total benign train loss : \", train_loss / total)\n",
    "    \n",
    "    \n",
    "\n",
    "def test(epoch):\n",
    "    print(\"\\n[ Test epoch : %d ]\" % epoch)\n",
    "    net.eval()\n",
    "    loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        total += targets.size(0)\n",
    "        \n",
    "        outputs = net(inputs)\n",
    "        loss += criterion(outputs, targets).item()\n",
    "        \n",
    "        _, predicted = outputs.max(1)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "    print(\"\\Total benign test accuracy : \", 100 * correct / total)\n",
    "    print(\"Total benign test loss : \", loss / total)\n",
    "    \n",
    "    state = {\n",
    "        'net' : net.state_dict()\n",
    "    }\n",
    "    \n",
    "    if not os.path.isdir('checkpoint'):\n",
    "        os.mkdir('checkpoint')\n",
    "    torch.save(state, './checkpoint/' + file_name)\n",
    "    print('Model Saved!')\n",
    "    \n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    lr = learning_rate\n",
    "    if epoch >= 100:\n",
    "        lr /= 10\n",
    "    elif epoch >= 150:\n",
    "        lr /= 10\n",
    "    \n",
    "    for param_group in optimizer.param_groups: # .param_groups...?\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee5a025",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38eabb72",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for epoch in range(1, 201):\n",
    "    adjust_learning_rate(optimizer, epoch)\n",
    "    train(epoch)\n",
    "    test(epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853a722a",
   "metadata": {},
   "source": [
    "## 참고"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815a6948",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torchvision\n",
    "import torchvision.models as models\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c99fab",
   "metadata": {},
   "source": [
    "Deep Residual Learning for Image Recognition\n",
    "\n",
    "Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n",
    "\n",
    "https://arxiv.org/abs/1512.03385\n",
    "\n",
    "\n",
    "Identity Mappings in Deep Residual Networks\n",
    "\n",
    "Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n",
    "\n",
    "https://arxiv.org/abs/1603.05027"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1651ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "models.resnet18()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf42ad87",
   "metadata": {},
   "outputs": [],
   "source": [
    "models.resnet50()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89b060e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Model\n",
    "# resnet18 = models.resnet18(pretrained=True)\n",
    "resnet18 = models.resnet18(weights=\"DEFAULT\") # 자동으로 pretrain 된 모델 download\n",
    "\n",
    "## Dataset\n",
    "to_tensor = torchvision.transforms.Compose(\n",
    "                [torchvision.transforms.ToTensor(),\n",
    "               torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])]\n",
    "                                          )\n",
    "\n",
    "cifar10 = torchvision.datasets.CIFAR10(root='./', download=True, transform=to_tensor)\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(cifar10, batch_size=8, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8a3045",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, data in enumerate(dataloader):\n",
    "    \n",
    "    img, gt = data\n",
    "    \n",
    "    print(img.shape)\n",
    "    \n",
    "    scores = resnet18(img)\n",
    "    \n",
    "    print(scores.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b872b61",
   "metadata": {},
   "source": [
    "## Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c9092e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: 1. replace the last FC layer for cifar10\n",
    "### Hint: 1000 -> 10\n",
    "model = resnet18(weig)\n",
    "\n",
    "\n",
    "## TODO: 2. fine tuning the last classifier (FC layer) using the cifar 10 training set.\n",
    "\n",
    "\n",
    "\n",
    "## TODO: 3. evaluation of the cifar 10 test set.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706d3b74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "42a3fd1d",
   "metadata": {},
   "source": [
    "## Other models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addaaeaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "models.resnet50()\n",
    "\n",
    "models.resnet101()\n",
    "\n",
    "models.resnet152()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd03c04",
   "metadata": {},
   "source": [
    "# Skip connection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa5dcae",
   "metadata": {},
   "source": [
    "https://pytorch.org/vision/0.8/_modules/torchvision/models/resnet.html\n",
    "\n",
    "\n",
    "Basic Block vs. Bottleneck Block"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e533ae",
   "metadata": {},
   "source": [
    "![](./data/4_resnet_basicblock_bottleneck.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989c74c4",
   "metadata": {},
   "source": [
    "- Bottleneck 구조가 Image Classification에는 좋다고 함\n",
    "- 다른 Task (Semantic Segmentation, Image 랩슨..?) 복자"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f733923",
   "metadata": {},
   "source": [
    "# Batch normalization , Layer Normalization,  Instance Normalization, and Group Normalization\n",
    "\n",
    "![](./data/group_norm.png) \n",
    "\n",
    "\n",
    "\n",
    "https://wandb.ai/wandb_fc/GroupNorm/reports/Group-Normalization-in-Pytorch-With-Examples---VmlldzoxMzU0MzMy\n",
    "\n",
    "\n",
    "\n",
    "https://pytorch.org/docs/stable/generated/torch.nn.GroupNorm.html\n",
    "\n",
    "- Batch norm : 한 channel에 대하여 normlize가 각각 됨\n",
    "- Layer normalize는 모든 channel을 한번에 normalize 하지만 하나의 instance에 대해서만 normlize 함\n",
    "    - Input Size가 정해져있을 때, RNN 같은 구조에서 효율적\n",
    "- Instance norm은 Batch norm과 똑같지만 하나의 instance에 대해서 진행\n",
    "    - Batch가 중요하지 않은 경우에 효과적, batch간 correlation이 없는 경우 \n",
    "- Group norm은 channel 별로 group을 나누어서 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db231c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torchvision\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f3a7d9",
   "metadata": {},
   "source": [
    "\n",
    "Batch norm, layer norm, instance norm, group norm\n",
    "\n",
    "https://pytorch.org/docs/stable/nn.html\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html#torch.nn.LayerNorm\n",
    "\n",
    "https://pytorch.org/docs/stable/generated/torch.nn.GroupNorm.html\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f752a728",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNorm(nn.Module):\n",
    "    def __init__(self, in_channel, out_channels):\n",
    "        super(BatchNorm, self).__init__()\n",
    "        self.bn = nn.BatchNorm2d(in_channel)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        out = self.bn(x)  #[N, C, HW] -> [N, C, HW]\n",
    "        \n",
    "        \n",
    "        return out\n",
    "\n",
    "    \n",
    "## For different sequences, e.g., RNN.\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, in_shape, out_channels):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.ln = nn.LayerNorm(in_shape, eps=1e-08)\n",
    "\n",
    "    def forward(self,x):\n",
    "        out = self.ln(x)  #[N, C, HW] -> [N, C, HW]\n",
    "\n",
    "        \n",
    "        return out\n",
    "\n",
    "    \n",
    "## For style transfer, domain adaptation.\n",
    "class InstanceNorm(nn.Module):\n",
    "    def __init__(self, in_channel, out_channels):\n",
    "        super(InstanceNorm, self).__init__()\n",
    "        self.In = nn.InstanceNorm2d(in_channel, eps=1e-08) \n",
    "\n",
    "    def forward(self,x):\n",
    "        out = self.In(x)  #[N, C, HW] -> [N, C, HW]\n",
    "        return out\n",
    "\n",
    "    \n",
    "## stable in small batch size.\n",
    "class GroupNorm(nn.Module):\n",
    "    def __init__(self, group_size, in_channel, out_channels):\n",
    "        super(GroupNorm, self).__init__()\n",
    "        self.gn = nn.GroupNorm(group_size, in_channel, eps=1e-08)  ## num_group and in_channel\n",
    "\n",
    "    def forward(self,x):\n",
    "        out = self.gn(x) #[N, C, HW] -> [N, C, HW]\n",
    "        \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59c88c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_channel = 64\n",
    "feature = torch.randn(8, in_channel, 120, 120)  ## temp tensor [B, C, H, W]\n",
    "\n",
    "\n",
    "BN = BatchNorm(in_channel, out_channels=64)\n",
    "\n",
    "out_feat = BN(feature)\n",
    "\n",
    "print(out_feat.shape) # shape은 똑같지만 batch 전체가 nomalize 된 값으로 나옴\n",
    "# 8개가 한번에 normalize 됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8db10c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "LN = LayerNorm(in_shape=list(feature.shape[1:]), out_channels=64)\n",
    "# Batch 단위로 실행되는것이 아니기 때문에 shape이 들어가야 함\n",
    "\n",
    "out_feat = LN(feature)\n",
    "\n",
    "print(out_feat.shape)  # 8개 각각이 따로 normalize 됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6e914b",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature.shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f032c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "IN=InstanceNorm(in_channel, out_channels=64)\n",
    "\n",
    "out_feat = IN(feature)\n",
    "\n",
    "print(out_feat.shape) # 8, 64 제외한 120, 120이 한번에 normalizaion 됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8382515b",
   "metadata": {},
   "outputs": [],
   "source": [
    "GN=GroupNorm(group_size=2, in_channel=in_channel, out_channels=64)\n",
    "\n",
    "out_feat = GN(feature)\n",
    "\n",
    "print(out_feat.shape)  ## 32 / 32, 32개씩 나눠진 channel 들이 normalize 됨\n",
    "\n",
    "GN=GroupNorm(group_size=4, in_channel=in_channel, out_channels=64)\n",
    "\n",
    "out_feat = GN(feature)\n",
    "\n",
    "print(out_feat.shape)  ## 16 / 16 / 16 / 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93f10a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
