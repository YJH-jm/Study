**참고자료**

[1. 1시간만에 GAN(Generative Adversarial Network) 완전 정복하기](https://www.youtube.com/watch?v=odpjk7_tGY0)

[2. CSC321 Lecture 19: GAN](http://www.cs.toronto.edu/~rgrosse/courses/csc321_2018/)

# GAN (Generative Adversarial Networks)
## Discriminative Model v.s. Generative Model
- Discriminative model
    
    <br>

    <p align=center><img src="images/image110.png" width = 60%></p>

    <br>

- Generative model
    - unsupervised model
        - 즉, input만 있음

    <br>

    <p align=center><img src="images/image111.png" width = 60%></p>

<br>
<br>

## Probability Distribution
<p align=center><img src="images/image112.png" width = 60%></p>
<br>

- event : 주사위 던지면 {1, 2, 3, 4, 5, 6}의 event 발생
- 각각의 1~6까지의 각각의 event를 1~6까지의 random variable이라고 assign
- discrite 한 경우이기 때문에 PMF라 함

- true 한 distribution이라고 한다면 무수히 많이 반복한다면 위와 비슷한 histogram을 가지게 됨
- 몇 번의 경험을 가지고 나온 확률이라면 통계적으로 이 확률을 추론을 할 수 있음 

<p align=center><img src="images/image113.png" width = 60%></p>

<br>
<br>

<p align=center><img src="images/image114.png" width = 60%></p>

- random variable &nbsp;<img src="https://latex.codecogs.com/svg.image?x" title="x" /> 가 training data의 image( <img src="https://latex.codecogs.com/svg.image?64\times&space;64&space;\times&space;3" title="64\times 64 \times 3" /> ) 인 경우

- 사람이라는 사전정보가 있으면 어느 정도 특정한 분포를 가짐
    - 눈의 위치, 얼굴의 위치 등

<br>

<p align=center><img src="images/image115.png" width = 60%></p>


- 얼굴 데이터의 확률 분포가 존재 할 것 (차원을 plot해서 볼 수 없어서 1차원이라고 가정)
- <img src="https://latex.codecogs.com/svg.image?P_{data}(x)" title="P_{data}(x)" />와 비슷한 <img src="https://latex.codecogs.com/svg.image?P_{model}(x)" title="P_{model}(x)" /> approximation 하여 그 모델에서 random sampling 하여 진짜처럼 보이는 이미지가 나오게 해야 함!

<br>
<br>

## Probability density function estimation problem
- <img src="https://latex.codecogs.com/svg.image?P_{model}(x)" title="P_{model}(x)" /> 가 <img src="https://latex.codecogs.com/svg.image?P_{data}(x)" title="P_{data}(x)" /> 비슷하면, <img src="https://latex.codecogs.com/svg.image?P_{model}(x)" title="P_{model}(x)" /> 에서 sampling 하여 데이터를 얻을 수 있음 

<br>

<p align=center><img src="images/image116.png" width = 60%></p>

<br>
<br>


## Generative models from lower dimension
- 실질적으로 분포를 구하는 것이 GAN의 목적
<p align=center><img src="images/image117.png" width = 60%></p>

<br>
<br>

## Deterministic Transformation by Network
- 다시 정리하자면, &nbsp; <img src="https://latex.codecogs.com/svg.image?P_{model}(x)" title="P_{model}(x)" /> 이 &nbsp; <img src="https://latex.codecogs.com/svg.image?P_{data}(x)" title="P_{data}(x)" /> 에 근사하도록 만드는 것이 GAN 목적
- Network 자체는 deterministic 함

<br>

- 1 dimensionla example
    <p align=center><img src="images/image118.png" width = 60%></p>

    - input neuron이 Gaussian 분포를 가짐
    - network는 모델을 학습하여 mapping을 진행

    - 만들어진 output이  &nbsp; <img src="https://latex.codecogs.com/svg.image?P_{data}(x)" title="P_{data}(x)" />  과 근사하다면 원래는 그 모델에서 sampling을 하지만 우리가 알고있는 Gaussian에서 sampling 하여 Network를 거쳐 Fake로 generate된 dataset 획득

<br>

- example
    <p align=center><img src="images/image119.png" width = 60%></p>

    - 각각 gaussian 분포를 가지는 input들이 학습된 Network와 mapping된 분포가 training 분포가 근사하다면
    
    <br>

    <p align=center><img src="images/image120.png" width = 60%></p>

    - 각각 input에 맞게 sampling한 데이터를 network에 mapping 시키면 train dataset과 근사한 fake로 generate된 dataset이 생성됨
<br>

- 1 dimensionla example
    <p align=center><img src="images/image118.png" width = 60%></p>

    - input neuron이 Gaussian 분포를 가짐
    - network는 모델을 학습하여 mapping을 진행

    - 만들어진 output이  &nbsp; <img src="https://latex.codecogs.com/svg.image?P_{data}(x)" title="P_{data}(x)" />  과 근사하다면 원래는 그 모델에서 sampling을 하지만 우리가 알고있는 Gaussian에서 sampling 하여 Network를 거쳐 Fake로 generate된 dataset 획득
**Network는 distribution을 만드는 것 아님**
**알고있는 distribution에서 target distribution으로 mapping 해주는 역할!**