**참고자료(이미지, 내용)**

[1. 1시간만에 GAN(Generative Adversarial Network) 완전 정복하기](https://www.youtube.com/watch?v=odpjk7_tGY0)

[2. CSC321 Lecture 19: GAN](http://www.cs.toronto.edu/~rgrosse/courses/csc321_2018/)

# Generative model
## Discriminative Model v.s. Generative Model
- Discriminative model
    
    <br>

    <p align=center><img src="images/image110.png" width = 60%></p>

    <br>

- Generative model
    - unsupervised model

    <br>

    <p align=center><img src="images/image111.png" width = 60%></p>

<br>
<br>

## Probability Distribution
<p align=center><img src="images/image112.png" width = 60%></p>
<br>

- event : 주사위 던지면 {1, 2, 3, 4, 5, 6}의 event 발생
- 각각의 event를 1~6까지의 random variable이라고 assign
- discrete 한 경우이기 때문에 PMF라 함

- True 한 distribution이라고 한다면 무수히 많이 반복한다면 위와 비슷한 histogram을 가지게 됨
- 몇 번의 경험을 가지고 나온 확률이라면 통계적으로 이 확률을 추론을 할 수 있음 

<p align=center><img src="images/image113.png" width = 60%></p>

<br>
<br>

<p align=center><img src="images/image114.png" width = 60%></p>

- random variable &nbsp;<img src="https://latex.codecogs.com/svg.image?x" title="x" /> 가 training data의 image( <img src="https://latex.codecogs.com/svg.image?64\times&space;64&space;\times&space;3" title="64\times 64 \times 3" /> ) 인 경우

- 사람이라는 사전정보가 있으면 어느 정도 특정한 분포를 가짐
    - 눈의 위치, 얼굴의 위치 등

<br>

<p align=center><img src="images/image115.png" width = 60%></p>


- 얼굴 데이터의 확률 분포가 존재 할 것 (차원을 plot해서 볼 수 없어서 1차원이라고 가정)
- <img src="https://latex.codecogs.com/svg.image?P_{data}(x)" title="P_{data}(x)" />와 비슷한 <img src="https://latex.codecogs.com/svg.image?P_{model}(x)" title="P_{model}(x)" /> approximation 하여 그 모델에서 random sampling 하여 진짜처럼 보이는 이미지가 나오게 해야 함!

<br>
<br>

## Probability density function estimation problem
- <img src="https://latex.codecogs.com/svg.image?P_{model}(x)" title="P_{model}(x)" /> 이 <img src="https://latex.codecogs.com/svg.image?P_{data}(x)" title="P_{data}(x)" /> 와 비슷하면, <img src="https://latex.codecogs.com/svg.image?P_{model}(x)" title="P_{model}(x)" /> 에서 sampling 하여 데이터를 얻을 수 있음 

<br>

<p align=center><img src="images/image116.png" width = 60%></p>

<br>
<br>


## Generative models from lower dimension
- 실질적으로 분포를 구하는 것이 목적
<p align=center><img src="images/image117.png" width = 60%></p>

<br>
<br>

## Deterministic Transformation by Network
- 다시 정리하자면, &nbsp; <img src="https://latex.codecogs.com/svg.image?P_{model}(x)" title="P_{model}(x)" /> 이 &nbsp; <img src="https://latex.codecogs.com/svg.image?P_{data}(x)" title="P_{data}(x)" /> 에 근사하도록 만드는 것이 Generative modeling 목적
- Network 자체는 deterministic 함

<br>

- 1 dimensionla example
    <p align=center><img src="images/image118.png" width = 60%></p>

    - input neuron이 Gaussian 분포를 가짐
    - network는 모델을 학습하여 mapping을 진행

    - 만들어진 output이  &nbsp; <img src="https://latex.codecogs.com/svg.image?P_{data}(x)" title="P_{data}(x)" />  과 근사하다면 원래는 그 모델에서 sampling을 하지만 우리가 알고있는 Gaussian에서 sampling 하여 Network를 거쳐 Fake로 generate된 dataset 획득

<br>

- example
    <p align=center><img src="images/image119.png" width = 60%></p>

    - 각각 gaussian 분포를 가지는 input들이 학습된 Network와 mapping된 분포가 training 분포가 근사하다면
    
    <br>

    <p align=center><img src="images/image120.png" width = 60%></p>

    - 각각 input에 맞게 sampling한 데이터를 network에 mapping 시키면 train dataset과 근사한 fake로 generate된 dataset이 생성됨

<br>

- example
    <p align=center><img src="images/image121.png" width = 60%></p>


<br>
<br>

# Generative Adversarial Networks (GAN)
- generative model은  <img src="https://latex.codecogs.com/svg.image?P_{model}(x)" title="P_{model}(x)" /> 이 &nbsp; <img src="https://latex.codecogs.com/svg.image?P_{data}(x)" title="P_{data}(x)" /> 에 근사하도록 만드는 network를 학습시키는 것
- GAN은 explicit density function을 사용하지 않음

## Adversarial Nets Framework
- 모델의 성능을 판단하는 방법 중 하나는 그 모델을 sampling 하는 것
- GAN은 기존의 generative modeling과 다른 관점
    - 진짜와 가짜를 구별하는 역할을 하는 discriminator network에서 판단하여 실제 데이터와 구별할 수 없는 샘플을 생성하는 모델

## Generative Adversarial Network
- idea : 2개의 다른 네트워크를 train
    - generator network 
        - 진짜 같은 sample을 만들어내기 위함
    - Discriminator network 
        - 진짜 data와 가짜 data를 구별하기 위함

- generator network는 discriminator network를 속이려고 노력
    - 처음에 discriminator는 generator가 만든 데이터 잘 구별
    - generator이 학습이 되면 진짜 같은 가짜가 데이터가 생성이 되게 되고 discriminator는 50%의 확률로 맞추게 됨


    <br>

    <p align=center><img src="images/image122.png" width = 60%></p>

    - Discrimiator 는 real data를 real로, fake data를 fake로 하게 하는 Loss function을 define 하여 minimize 하게 함
    - Generator는 fake로 만들어진 generate 한 데이터가 discriminator에 들어가서 real 이라고 나오게 만들게 Loss function define 

    <br>
    <br>

## Discriminator Perspective

<p align=center><img src="images/image123.png" width = 60%></p>

<p align=center><img src="images/image124.png" width = 60%></p>

<br>
<br>


## Generator Perspective

<p align=center><img src="images/image125.png" width = 60%></p>


<br>
<br>

## Loss Function of Discriminator 

<p align=center><img src="images/image126.png" width = 60%></p>

<br>
<br>

## Loss Functoin of Generator

<p align=center><img src="images/image127.png" width = 60%></p>

<br>
<br>

## Non-Saturating Game

<p align=center><img src="images/image128.png" width = 60%></p>

- Generator gradient가 작아 초반에 학습이 잘 되지 않음

<br>

<p align=center><img src="images/image129.png" width = 60%></p>

<br>
<br>

## Solving a Problem
### 1
- step 1 : &nbsp;<img src="https://latex.codecogs.com/svg.image?G" title="G" /> 를 고정하고 gradeint step을 수행

<p align=center><img src="https://latex.codecogs.com/svg.image?\max_{D}&space;E_{x&space;\sim&space;p_{\text{data}}(x)}\left[\log&space;D(x)\right]&space;&space;&plus;&space;E_{x&space;\sim&space;p_{z}(z)}\left[\log&space;(1-D(G(z)))\right]" title="\max_{D} E_{x \sim p_{\text{data}}(x)}\left[\log D(x)\right] + E_{x \sim p_{z}(z)}\left[\log (1-D(G(z)))\right]" /></p>

- step 2 : &nbsp;<img src="https://latex.codecogs.com/svg.image?D" title="D" />를 고정하고 gradient step 수행 

<p align=center><img src="https://latex.codecogs.com/svg.image?\max_{G}&space;E_{x&space;\sim&space;p_{z}(z)}\left[\log&space;D(G(z))\right]" title="\max_{G} E_{x \sim p_{z}(z)}\left[\log D(G(z))\right]" /></p>
<p align=center></p>

<br>

### 2
- step 1 : &nbsp;<img src="https://latex.codecogs.com/svg.image?G" title="G" /> 를 고정하고 gradeint step을 수행

<p align=center><img src="https://latex.codecogs.com/svg.image?\min_{D}&space;E_{x&space;\sim&space;p_{\text{data}}(x)}\left[-\log&space;D(x)\right]&space;&space;&plus;&space;E_{x&space;\sim&space;p_{z}(z)}\left[-\log&space;(1-D(G(z)))\right]" title="\min_{D} E_{x \sim p_{\text{data}}(x)}\left[-\log D(x)\right] + E_{x \sim p_{z}(z)}\left[-\log (1-D(G(z)))\right]" /></p>

- step 2 : &nbsp;<img src="https://latex.codecogs.com/svg.image?D" title="D" />를 고정하고 gradient step 수행 

<p align=center><img src="https://latex.codecogs.com/svg.image?\min_{G}&space;E_{x&space;\sim&space;p_{z}(z)}\left[-\log&space;D(G(z))\right]" title="\min_{G} E_{x \sim p_{z}(z)}\left[-\log D(G(z))\right]" /></p>

