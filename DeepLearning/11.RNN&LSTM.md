# 순환신경망 (RNN - Recurrent Neural Network)
- Sequence Data(순차데이터) 분석을 위한 신경망

<br>
<br>


## Sequence Data(순차데이터) 
- 순서가 의미가 있으며, 순서가 달라질 경우 의미가 바뀌거나 손상되는 데이터

<p align=center><img src="images/image80.png" width=60%></p>

<br>
<br>

##  Sequence Data 분석
-  비시계열 데이터 모델 (ANN, CNN)

    <p align=center><img src="images/image134.png" width=40%> </p>
    
    <center><a href="https://i-systems.github.io/teaching/DL/iNotes_tf2/17_RNN_tf2.html"> 출처 </a> </center>

    <br>

    - 각각의 time index 서로 독립적으로 움직임
        - 과거의 data가 현재의 data를 결정하는 것에 아무런 영향이 없다는 가정
    
    <br>

- 시계열 데이터 모델
    - 시계열 데이터의 가장 큰 특징인 시간 흐름 방향의 dynamic한 behavior property 파악 가능한 구조
        - 이 특징을 hidden state(layer)에 둠
        - state가 중요한 정보를 가지고 있고, 그 정보가 시간에 따른 특성들을 표현할 수 있음

    <br>

    <p align=center><img src="images/image135.png" width=40%> </p>
    
    <center><a href="https://i-systems.github.io/teaching/DL/iNotes_tf2/17_RNN_tf2.html"> 출처 </a> </center>

    <br>

<br>
<br>

### Sequence-to-Vector (many to one)

- Sequence가 들어가면 Vector(하나)가 출력 

- 예: 주가예측 : 4일간의 주가가 들어가면 그 다음날 주가가 나옴

<p align=center><img src="images/image81.png" width=40%></p>

- Input: 4일간 주가
- Output: 5일째 주가

<br>

<p align=center><img src="images/image82.png" width=40%></p>

<br>

### Sequence-to-Sequence(many to many)
- 예: Machine translation (번역)

<p align=center><img src="images/image83.png" width=40%></p>

- Input: 영어 문장 전체를 단어 단위로 순차적 입력
- Output: 번역된 한글 문장을 단어 단위로 순차적 출력

<br>

<p align=center><img src="images/image84.png" width=40%></p>

<br>

### Vector-to-Sequence (one to many)
- 이미지 하나(Vector)가 들어가면 단어들(문장) 출력
- 예: Image captioning 
    - 이미지를 설명하는 문장을 만드는 것 
    - 하나의 그림에 문장(단어들)이 나옴

<p align=center><img src="images/image85.png" width=40%></p>


- Input: 이미지
- Output: 이미지에 대한 설명을 단어 단위로 순차적으로 출력 

<br>

<p align=center><img src="images/image86.png" width=40%></p>

<br>
<br>
<br>

# RNN (Recurrent neural networks) 개요
- t에서의 값을 알기 위해 t-1의 값을 사용하는 것을 Recurrence 하다고 함

<br>
<br>

## RNN Shotcut
- Input : 시계열 데이터의 window size 만큼의 데이터
- 각각의 layer들은 많은 뉴런의 수를 가짐
- 이를 하나의 box로 간단하게 표현

    <br>

    <p  align=center><img src = "images/image133.png" width=60%></p>
    <center><a href="https://i-systems.github.io/teaching/DL/iNotes_tf2/17_RNN_tf2.html"> 출처 </a> </center>



<br>
<br>

## Memory System (기억시스템)
- **window size**
    - window size = n이면 t의 데이터가 t, t+1, t+2, ... ,t-(n-1) 에는 영향을 미치지만 그 후에는 미치지 않는 다는 가정 또는 의미
    - 이론상으로는 n이 무한대이면 가장 이상적이나 현실적으로 불가능 

<br>

<p align=center><img src="images/image87.png" width=60%></p>

<br>

- 4일간의 주가 변화로 5일째 주가를 예측하려면 입력받은 4일간의 주가를 순서 기억해야 함
    - window size = 4
    - t의 데이터가 t, t+1, t+2, t+3에는 영향을 미치지만 그 후에는 미치지 않는 다는 것을 의미
- Fully Connected Layer나 Convolution Layer의 출력은 이전 Data에 대한 출력에 영향을 받지 않음

<br>
<br>

## Simple RNN
<br>

<p align=center><img src="images/image136.png" width="60%"></p>
<center><a href="https://i-systems.github.io/teaching/DL/iNotes_tf2/17_RNN_tf2.html"> 출처 </a> </center>

<br>

- RNN은 내부에 반복(Loop)를 가진 신경망의 한 종류
- 각 입력 데이터는 순서대로 들어오며 Node/Unit은 **입력데이터(<img src="https://latex.codecogs.com/svg.image?x" title="x" />)** 와 **이전 입력에 대한 출력 데이터(<img src="https://latex.codecogs.com/svg.image?h_{n-1}" title="h_{n-1}" />)** 를 같이 입력 받음
- 입력 데이터에 weight를 가중합한 값과 이전 입력에 대한 출력 값에 weight를 가중한 값을 더해 activation을 통과한 값이 출력값 
- 그 출력값을 다음 Sequence 데이터 처리에 전달

<br>


<!-- <p align=center><img src="images/image88.png" width=60%></p> -->

<p align=center><img src="images/image89.png" width=60%></p>

<p align=center><img src="https://latex.codecogs.com/svg.image?h_t&space;=&space;tanh(W_{xh}&space;*&space;X_t&space;&plus;&space;W_{hh}&space;*&space;h_{t-1}&space;&plus;&space;b_t)" title="h_t = tanh(W_{xh} * X_t + W_{hh} * h_{t-1} + b_t)" /></p>
<p align=center><img src="https://latex.codecogs.com/svg.image?y_{t}=W_{hy}*h_t&plus;b_{y}" title="y_{t}=W_{hy}*h_t+b_{y}" /></p>
<br>

<p align=center><img src="images/image90.png" width=25%></p>

<!-- - 반복문을 돌면서 처리됨 -->

<br>
<br>

### RNN의 문제
- 순환을 하기 때문에 학습해야 할 parameter의 수가 훨씬 더 많아지고 network가 복잡해짐
    - 학습이 상대적으로 어려움
- Sequence가 긴 경우 앞쪽의 기억이 뒤쪽에 영향을 미치지 못함
    - 학습능력이 떨어짐
    - 경사 소실(Gradient Vanishing) 문제로 처음의 input값이 점점 잊혀지는 현상 발생 
- ReLU activation, parameter initialization의 조정 등 보다 모형의 **구조적**으로 해결하려는 시도 
    - **Long Short Term Memory(LSTM; Hochreiter & Schmidhuber, 1997)**
    - Gated Recurrent Unit(GRU; Kyunghyun Cho et al., 2014) 
        - LSTM의 성능을 최대한 유지하려고 하면서 계산량은 줄여줌
        - LSTM보다 성능은 떨어짐


<br>
<br>

## LSTM (Long Short Term Memory)
- RNN을 개선한 변형 알고리즘
    - 바로 전 time step(short term)의 처리 결과와 전체 time step(long term)의 처리 결과를 같이 입력 받음
- 오래 기억할 것은 유지하고 잊어버릴 것은 빨리 잊어버리자

<br>

<p align=center><img src="images/image91.png" width=60%></p>

<br>

- LSTM의 노드는 RNN의 hidden state에 Cell state 를 추가로 출력

<br>

- Cell State 
    - 장기기억
    - 기억을 오래 유지하기 위해 전달 하는 값
    - 이전 노드들에의 출력 값에 현재 입력에 대한 값을 더함

<p align=center><img src="images/image92.png" width=60%></p>

<br>

### LSTM의 구조
- **Forget gate**
- **Input gate**
- **output gate**

<p align=center><img src="images/image93.png" width=60%></p>

<br>

### Forget gate
- '과거 정보를 잊기’를 위한 게이트
- cell state의 값을 줄이는 역할
- 현재 노드의 입력값을 기준으로 Cell state의 값에서 **얼마나 잊을지** 결정
- 현재 데이터가 중요한 경우는 현재데이터를 많이 반영
- 현재 데이터가 중요하지 않은 경우는 과거의 데이터를 많이 반영
- 이 값이 0에 가까울수록 정보가 많이 삭제된 것이고 1에 가까울수록 정보를 온전히 기억한 것!
    - 1에 가까울수록 과거 정보 많이 기억
    - 0에 가까울수록 과거 정보 많이 잃음
    
    <br>

<p align=center><img src="images/image94.png" width=60%></p>

<p align=center><img src="https://latex.codecogs.com/svg.image?\begin{align*}{&space;f&space;}_{&space;t&space;}&=\sigma&space;({&space;W&space;}_{&space;xh\_&space;f&space;}{&space;x&space;}_{&space;t&space;}&plus;{&space;W&space;}_{&space;hh\_&space;f&space;}{&space;h&space;}_{&space;t-1&space;}&plus;{&space;b&space;}_{&space;h\_&space;f&space;})\end{align*}" title="\begin{align*}{ f }_{ t }&=\sigma ({ W }_{ xh\_ f }{ x }_{ t }+{ W }_{ hh\_ f }{ h }_{ t-1 }+{ b }_{ h\_ f })\end{align*}" /></p>

<BR>

### Input gate
- ‘현재 정보를 기억하기’ 위한 게이트
- 현재 노드의 입력값을 Cell state에 추가
    - 즉, cell state에 현재입력을 더해주는 것

    <br>

<p align=center><img src="images/image95.png" width=60%></p>

<p align=center><img src="https://latex.codecogs.com/svg.image?\begin{align*}{&space;i&space;}_{&space;t&space;}&=\sigma&space;({&space;W&space;}_{&space;xh\_&space;i&space;}{&space;x&space;}_{&space;t&space;}&plus;{&space;W&space;}_{&space;hh\_&space;i&space;}{&space;h&space;}_{&space;t-1&space;}&plus;{&space;b&space;}_{&space;h\_&space;i&space;})\end{align*}" title="\begin{align*}{ i }_{ t }&=\sigma ({ W }_{ xh\_ i }{ x }_{ t }+{ W }_{ hh\_ i }{ h }_{ t-1 }+{ b }_{ h\_ i })\end{align*}" /></p>

<p align=center><img src="https://latex.codecogs.com/svg.image?\begin{align*}{&space;g&space;}_{&space;t&space;}&=\tanh&space;{&space;({&space;W&space;}_{&space;xh\_&space;g&space;}{&space;x&space;}_{&space;t&space;}&plus;{&space;W&space;}_{&space;hh\_&space;g&space;}{&space;h&space;}_{&space;t-1&space;}&plus;{&space;b&space;}_{&space;h\_&space;g&space;})&space;}&space;\end{align*}" title="\begin{align*}{ g }_{ t }&=\tanh { ({ W }_{ xh\_ g }{ x }_{ t }+{ W }_{ hh\_ g }{ h }_{ t-1 }+{ b }_{ h\_ g }) } \end{align*}" /></p>

<p align=center><img src="https://latex.codecogs.com/svg.image?\begin{align*}{&space;i&space;}_{&space;t&space;}\odot&space;{&space;g&space;}_{&space;t&space;}\end{align*}" title="\begin{align*}{ i }_{ t }\odot { g }_{ t }\end{align*}" /></p>


<br>

### Cell State 업데이트
- forget gate의 결과를 곱하고 input gate의 결과를 더함
    - 이전 메모리에 현재 입력으로 대체되는 것을 지우고 현재 입력의 결과를 더함

<br>

<p align=center><img src="images/image96.png" width=60%></p>

<p align=center><img src="https://latex.codecogs.com/svg.image?\begin{align*}{&space;c&space;}_{&space;t&space;}&={&space;f&space;}_{&space;t&space;}\odot&space;{&space;c&space;}_{&space;t-1&space;}&plus;{&space;i&space;}_{&space;t&space;}\odot&space;{&space;g&space;}_{&space;t&space;}\end{align*}" title="\begin{align*}{ c }_{ t }&={ f }_{ t }\odot { c }_{ t-1 }+{ i }_{ t }\odot { g }_{ t }\end{align*}" /></p>

<br>

### Output gate
- LSTM에서 output은 hidden state 
- 다음 Input Data를 처리하는 Cell로 전달

<br>

<p align=center><img src="images/image97.png" width=60%></p>

<p align=center><img src="https://latex.codecogs.com/svg.image?\begin{align*}{&space;o&space;}_{&space;t&space;}&=\sigma&space;({&space;W&space;}_{&space;xh\_&space;o&space;}{&space;x&space;}_{&space;t&space;}&plus;{&space;W&space;}_{&space;hh\_&space;o&space;}{&space;h&space;}_{&space;t-1&space;}&plus;{&space;b&space;}_{&space;h\_&space;o&space;})\end{align*}" title="\begin{align*}{ o }_{ t }&=\sigma ({ W }_{ xh\_ o }{ x }_{ t }+{ W }_{ hh\_ o }{ h }_{ t-1 }+{ b }_{ h\_ o })\end{align*}" /></p>
<p align=center><img src="https://latex.codecogs.com/svg.image?\begin{align*}{&space;h&space;}_{&space;t&space;}&={&space;o&space;}_{&space;t&space;}\odot&space;\tanh&space;{&space;({&space;c&space;}_{&space;t&space;})&space;}&space;\end{align*}" title="\begin{align*}{ h }_{ t }&={ o }_{ t }\odot \tanh { ({ c }_{ t }) } \end{align*}" /></p>


